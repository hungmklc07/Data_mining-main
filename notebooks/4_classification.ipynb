{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classification - Ph√¢n lo·∫°i c·∫ßu th·ªß v√† ƒë·ªôi b√≥ng\n",
        "\n",
        "Notebook n√†y th·ª±c hi·ªán ph√¢n lo·∫°i s·ª≠ d·ª•ng Random Forest v√† Decision Tree.\n",
        "\n",
        "## M·ª•c ti√™u:\n",
        "1. Ph√¢n lo·∫°i v·ªã tr√≠ c·∫ßu th·ªß\n",
        "2. Ph√¢n lo·∫°i ƒë·ªôi b√≥ng v√†o Top 4\n",
        "3. Ph√¢n lo·∫°i hi·ªáu su·∫•t c·∫ßu th·ªß\n",
        "4. ƒê√°nh gi√° v√† so s√°nh models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sys.path.append('../src')\n",
        "from classification import (\n",
        "    classify_player_position, classify_team_top4, classify_player_performance,\n",
        "    evaluate_classification, get_feature_importance\n",
        ")\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "\n",
        "print(\"‚úÖ ƒê√£ import c√°c modules c·∫ßn thi·∫øt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Ph√¢n lo·∫°i v·ªã tr√≠ c·∫ßu th·ªß\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load d·ªØ li·ªáu\n",
        "try:\n",
        "    players_df = pd.read_excel('../data/players_processed.xlsx')\n",
        "except:\n",
        "    from data_preprocessing import load_data, feature_engineering_players, prepare_data_for_analysis\n",
        "    data = load_data()\n",
        "    players_df = feature_engineering_players(data['players'])\n",
        "    players_df = prepare_data_for_analysis(players_df)\n",
        "\n",
        "# Ph√¢n lo·∫°i v·ªã tr√≠\n",
        "print(\"=\"*70)\n",
        "print(\"PH√ÇN LO·∫†I V·ªä TR√ç C·∫¶U TH·ª¶\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "position_results = classify_player_position(players_df, min_samples_per_class=10)\n",
        "\n",
        "if position_results:\n",
        "    # ƒê√°nh gi√° Random Forest\n",
        "    rf_metrics = evaluate_classification(position_results['random_forest'], 'Random Forest')\n",
        "    print(f\"\\nüìä Random Forest Metrics:\")\n",
        "    for metric, value in rf_metrics.items():\n",
        "        print(f\"  {metric}: {value:.3f}\")\n",
        "    \n",
        "    # ƒê√°nh gi√° Decision Tree\n",
        "    dt_metrics = evaluate_classification(position_results['decision_tree'], 'Decision Tree')\n",
        "    print(f\"\\nüìä Decision Tree Metrics:\")\n",
        "    for metric, value in dt_metrics.items():\n",
        "        print(f\"  {metric}: {value:.3f}\")\n",
        "    \n",
        "    # Feature importance\n",
        "    rf_importance = get_feature_importance(\n",
        "        position_results['random_forest']['model'],\n",
        "        position_results['random_forest']['feature_names'],\n",
        "        top_n=15\n",
        "    )\n",
        "    \n",
        "    if rf_importance is not None:\n",
        "        print(\"\\nüìà Top 15 Features (Random Forest):\")\n",
        "        print(rf_importance)\n",
        "        \n",
        "        # Visualize\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "        \n",
        "        # Feature importance\n",
        "        ax1 = axes[0]\n",
        "        top_features = rf_importance.head(10)\n",
        "        ax1.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
        "        ax1.set_yticks(range(len(top_features)))\n",
        "        ax1.set_yticklabels(top_features['feature'], fontsize=9)\n",
        "        ax1.set_xlabel('Importance', fontweight='bold')\n",
        "        ax1.set_title('Top 10 Feature Importance (Random Forest)', fontweight='bold')\n",
        "        ax1.invert_yaxis()\n",
        "        \n",
        "        # Confusion Matrix\n",
        "        ax2 = axes[1]\n",
        "        cm = confusion_matrix(position_results['random_forest']['y_test'], \n",
        "                             position_results['random_forest']['predictions'])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
        "        ax2.set_xlabel('Predicted', fontweight='bold')\n",
        "        ax2.set_ylabel('Actual', fontweight='bold')\n",
        "        ax2.set_title('Confusion Matrix (Random Forest)', fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('../results/classification/position_classification.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Ph√¢n lo·∫°i ƒë·ªôi b√≥ng Top 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load d·ªØ li·ªáu ƒë·ªôi b√≥ng\n",
        "try:\n",
        "    teams_df = pd.read_excel('../data/teams_processed.xlsx')\n",
        "except:\n",
        "    from data_preprocessing import load_data, feature_engineering_teams, prepare_data_for_analysis\n",
        "    data = load_data()\n",
        "    teams_merged = feature_engineering_teams(data['teams_for'], data['teams_vs'])\n",
        "    if teams_merged is not None:\n",
        "        teams_df = prepare_data_for_analysis(teams_merged, target_cols=['Squad'])\n",
        "    else:\n",
        "        teams_df = None\n",
        "\n",
        "if teams_df is not None:\n",
        "    print(\"=\"*70)\n",
        "    print(\"PH√ÇN LO·∫†I ƒê·ªòI B√ìNG TOP 4\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    top4_results = classify_team_top4(teams_df)\n",
        "    \n",
        "    if top4_results:\n",
        "        # ƒê√°nh gi√°\n",
        "        rf_metrics = evaluate_classification(top4_results['random_forest'], 'Random Forest')\n",
        "        dt_metrics = evaluate_classification(top4_results['decision_tree'], 'Decision Tree')\n",
        "        \n",
        "        print(f\"\\nüìä Random Forest Metrics:\")\n",
        "        for metric, value in rf_metrics.items():\n",
        "            print(f\"  {metric}: {value:.3f}\")\n",
        "        \n",
        "        print(f\"\\nüìä Decision Tree Metrics:\")\n",
        "        for metric, value in dt_metrics.items():\n",
        "            print(f\"  {metric}: {value:.3f}\")\n",
        "        \n",
        "        # Visualize\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Confusion Matrix RF\n",
        "        ax1 = axes[0]\n",
        "        cm_rf = confusion_matrix(top4_results['random_forest']['y_test'], \n",
        "                                 top4_results['random_forest']['predictions'])\n",
        "        sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=ax1, \n",
        "                   xticklabels=['Not Top 4', 'Top 4'], yticklabels=['Not Top 4', 'Top 4'])\n",
        "        ax1.set_title('Confusion Matrix - Random Forest', fontweight='bold')\n",
        "        \n",
        "        # Confusion Matrix DT\n",
        "        ax2 = axes[1]\n",
        "        cm_dt = confusion_matrix(top4_results['decision_tree']['y_test'], \n",
        "                                top4_results['decision_tree']['predictions'])\n",
        "        sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Oranges', ax=ax2,\n",
        "                   xticklabels=['Not Top 4', 'Top 4'], yticklabels=['Not Top 4', 'Top 4'])\n",
        "        ax2.set_title('Confusion Matrix - Decision Tree', fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('../results/classification/top4_classification.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªôi b√≥ng\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ph√¢n lo·∫°i hi·ªáu su·∫•t\n",
        "print(\"=\"*70)\n",
        "print(\"PH√ÇN LO·∫†I HI·ªÜU SU·∫§T C·∫¶U TH·ª¶\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "performance_results = classify_player_performance(players_df)\n",
        "\n",
        "if performance_results:\n",
        "    # ƒê√°nh gi√°\n",
        "    rf_metrics = evaluate_classification(performance_results['random_forest'], 'Random Forest')\n",
        "    dt_metrics = evaluate_classification(performance_results['decision_tree'], 'Decision Tree')\n",
        "    \n",
        "    print(f\"\\nüìä Random Forest Metrics:\")\n",
        "    for metric, value in rf_metrics.items():\n",
        "        print(f\"  {metric}: {value:.3f}\")\n",
        "    \n",
        "    print(f\"\\nüìä Decision Tree Metrics:\")\n",
        "    for metric, value in dt_metrics.items():\n",
        "        print(f\"  {metric}: {value:.3f}\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    cm = confusion_matrix(performance_results['random_forest']['y_test'], \n",
        "                         performance_results['random_forest']['predictions'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', ax=ax)\n",
        "    le = performance_results['random_forest']['label_encoder']\n",
        "    class_names = le.classes_\n",
        "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
        "    ax.set_yticklabels(class_names, rotation=0)\n",
        "    ax.set_xlabel('Predicted', fontweight='bold')\n",
        "    ax.set_ylabel('Actual', fontweight='bold')\n",
        "    ax.set_title('Confusion Matrix - Performance Classification', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../results/classification/performance_classification.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ classification\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
