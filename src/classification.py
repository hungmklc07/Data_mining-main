"""
Module th·ª±c hi·ªán Classification (Random Forest v√† Decision Tree)
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

def prepare_classification_data(df, target_col, feature_keywords=None, exclude_cols=None):
    """
    Chu·∫©n b·ªã d·ªØ li·ªáu cho classification
    
    Parameters:
    -----------
    df : DataFrame
        D·ªØ li·ªáu g·ªëc
    target_col : str
        T√™n c·ªôt target
    feature_keywords : list
        Keywords ƒë·ªÉ ch·ªçn features
    exclude_cols : list
        C√°c c·ªôt c·∫ßn lo·∫°i b·ªè
    
    Returns:
    --------
    X, y : Features v√† target
    feature_names : List t√™n features
    """
    if exclude_cols is None:
        exclude_cols = ['Player', 'Nation', 'Squad', 'Born', 'Team']
    
    if feature_keywords is None:
        feature_keywords = ['gls', 'ast', 'xg', 'xa', 'sh', 'sot', 'pass', 'tkl', 'touches', 
                           'prg', 'sca', 'gca', 'int', 'blocks', 'carries']
    
    # Ch·ªçn features
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    feature_cols = []
    
    for col in numeric_cols:
        if col not in exclude_cols and col != target_col:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in feature_keywords):
                if '%' not in col and 'category' not in col_lower:
                    feature_cols.append(col)
    
    # L·ªçc d·ªØ li·ªáu
    df_clean = df[[target_col] + feature_cols].dropna()
    
    X = df_clean[feature_cols].fillna(0)
    y = df_clean[target_col]
    
    return X, y, feature_cols

def classify_player_position(df, min_samples_per_class=10):
    """
    Ph√¢n lo·∫°i v·ªã tr√≠ c·∫ßu th·ªß d·ª±a tr√™n c√°c ch·ªâ s·ªë
    
    Parameters:
    -----------
    df : DataFrame
        D·ªØ li·ªáu c·∫ßu th·ªß
    min_samples_per_class : int
        S·ªë m·∫´u t·ªëi thi·ªÉu m·ªói class
    
    Returns:
    --------
    dict v·ªõi k·∫øt qu·∫£
    """
    if 'Pos' not in df.columns:
        print("‚ö†Ô∏è Kh√¥ng c√≥ c·ªôt v·ªã tr√≠ (Pos)")
        return None
    
    # L·ªçc c√°c v·ªã tr√≠ c√≥ ƒë·ªß m·∫´u
    pos_counts = df['Pos'].value_counts()
    valid_positions = pos_counts[pos_counts >= min_samples_per_class].index.tolist()
    
    if len(valid_positions) == 0:
        print("‚ö†Ô∏è Kh√¥ng c√≥ v·ªã tr√≠ n√†o c√≥ ƒë·ªß m·∫´u")
        return None
    
    df_filtered = df[df['Pos'].isin(valid_positions)].copy()
    print(f"üìä Ph√¢n lo·∫°i v·ªã tr√≠ v·ªõi {len(valid_positions)} classes: {valid_positions}")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    X, y, feature_cols = prepare_classification_data(df_filtered, 'Pos')
    
    # Encode target
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)
    
    # Train models
    results = {}
    
    # Random Forest
    rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    
    results['random_forest'] = {
        'model': rf,
        'predictions': y_pred_rf,
        'y_test': y_test,
        'y_train': y_train,
        'label_encoder': le,
        'feature_names': feature_cols
    }
    
    # Decision Tree
    dt = DecisionTreeClassifier(random_state=42, max_depth=10)
    dt.fit(X_train, y_train)
    y_pred_dt = dt.predict(X_test)
    
    results['decision_tree'] = {
        'model': dt,
        'predictions': y_pred_dt,
        'y_test': y_test,
        'y_train': y_train,
        'label_encoder': le,
        'feature_names': feature_cols
    }
    
    return results

def classify_team_top4(teams_df):
    """
    Ph√¢n lo·∫°i ƒë·ªôi b√≥ng v√†o Top 4
    
    Parameters:
    -----------
    teams_df : DataFrame
        D·ªØ li·ªáu ƒë·ªôi b√≥ng
    
    Returns:
    --------
    dict v·ªõi k·∫øt qu·∫£
    """
    # T√¨m c·ªôt Points
    pts_cols = [c for c in teams_df.columns if 'pts' in c.lower() and 'category' not in c.lower()]
    
    if len(pts_cols) == 0:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt Points")
        return None
    
    pts_col = pts_cols[0]
    
    # T·∫°o target: Top 4 (1) ho·∫∑c kh√¥ng (0)
    teams_df = teams_df.copy()
    teams_df['Top4'] = (teams_df[pts_col].rank(ascending=False) <= 4).astype(int)
    
    print(f"üìä Ph√¢n lo·∫°i Top 4: {teams_df['Top4'].sum()} ƒë·ªôi trong Top 4")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    feature_keywords = ['gf', 'ga', 'xg', 'xga', 'pts', 'gd']
    X, y, feature_cols = prepare_classification_data(teams_df, 'Top4', feature_keywords=feature_keywords)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
    
    # Train models
    results = {}
    
    # Random Forest
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    
    results['random_forest'] = {
        'model': rf,
        'predictions': y_pred_rf,
        'y_test': y_test,
        'feature_names': feature_cols
    }
    
    # Decision Tree
    dt = DecisionTreeClassifier(random_state=42, max_depth=5)
    dt.fit(X_train, y_train)
    y_pred_dt = dt.predict(X_test)
    
    results['decision_tree'] = {
        'model': dt,
        'predictions': y_pred_dt,
        'y_test': y_test,
        'feature_names': feature_cols
    }
    
    return results

def classify_player_performance(df):
    """
    Ph√¢n lo·∫°i hi·ªáu su·∫•t c·∫ßu th·ªß: Elite, Good, Average, Below Average
    
    Parameters:
    -----------
    df : DataFrame
        D·ªØ li·ªáu c·∫ßu th·ªß
    
    Returns:
    --------
    dict v·ªõi k·∫øt qu·∫£
    """
    # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p
    score_cols = []
    for keyword in ['gls', 'ast', 'xg', 'xa']:
        cols = [c for c in df.columns if keyword in c.lower() and 'category' not in c.lower() and 'per' not in c.lower()]
        if cols:
            score_cols.append(cols[0])
    
    if len(score_cols) == 0:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c√°c ch·ªâ s·ªë ph√π h·ª£p")
        return None
    
    df = df.copy()
    df['Performance_Score'] = df[score_cols].sum(axis=1)
    
    # Ph√¢n lo·∫°i d·ª±a tr√™n quantiles
    q1 = df['Performance_Score'].quantile(0.25)
    q2 = df['Performance_Score'].quantile(0.50)
    q3 = df['Performance_Score'].quantile(0.75)
    
    def classify_perf(score):
        if score >= q3:
            return 'Elite'
        elif score >= q2:
            return 'Good'
        elif score >= q1:
            return 'Average'
        else:
            return 'Below_Average'
    
    df['Performance_Class'] = df['Performance_Score'].apply(classify_perf)
    
    print(f"üìä Ph√¢n lo·∫°i hi·ªáu su·∫•t:")
    print(df['Performance_Class'].value_counts())
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    feature_keywords = ['gls', 'ast', 'xg', 'xa', 'sh', 'sot', 'pass', 'tkl']
    X, y, feature_cols = prepare_classification_data(df, 'Performance_Class', feature_keywords=feature_keywords)
    
    # Encode target
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)
    
    # Train models
    results = {}
    
    # Random Forest
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X_train, y_train)
    y_pred_rf = rf.predict(X_test)
    
    results['random_forest'] = {
        'model': rf,
        'predictions': y_pred_rf,
        'y_test': y_test,
        'label_encoder': le,
        'feature_names': feature_cols
    }
    
    # Decision Tree
    dt = DecisionTreeClassifier(random_state=42, max_depth=10)
    dt.fit(X_train, y_train)
    y_pred_dt = dt.predict(X_test)
    
    results['decision_tree'] = {
        'model': dt,
        'predictions': y_pred_dt,
        'y_test': y_test,
        'label_encoder': le,
        'feature_names': feature_cols
    }
    
    return results

def evaluate_classification(results, model_name):
    """
    ƒê√°nh gi√° k·∫øt qu·∫£ classification
    
    Parameters:
    -----------
    results : dict
        K·∫øt qu·∫£ t·ª´ model
    model_name : str
        T√™n model
    
    Returns:
    --------
    dict v·ªõi c√°c metrics
    """
    y_test = results['y_test']
    y_pred = results['predictions']
    
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
        'recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),
        'f1': f1_score(y_test, y_pred, average='weighted', zero_division=0)
    }
    
    return metrics

def get_feature_importance(model, feature_names, top_n=15):
    """
    L·∫•y feature importance t·ª´ model
    
    Parameters:
    -----------
    model : Model object
    feature_names : list
        T√™n c√°c features
    top_n : int
        S·ªë features top
    
    Returns:
    --------
    DataFrame v·ªõi feature importance
    """
    if hasattr(model, 'feature_importances_'):
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False).head(top_n)
        return importance_df
    return None


